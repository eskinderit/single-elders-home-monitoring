{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from operator import mul\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parallel_region(data, func, *params):\n",
    "    return data.map(func)\n",
    "\n",
    "\n",
    "def data_parallel_region_with_reduction(data, func, reduce_func, *params):\n",
    "    prods = data.map(func)    \n",
    "    print(\"*params = \", *params)\n",
    "    print(\"params = \", params)\n",
    "    print(\"params[0] = \", params[0])\n",
    "    return prods.fold(*params, reduce_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_prod(factors):\n",
    "    prod = 1\n",
    "    for x in factors:\n",
    "        prod *= x\n",
    "    return prod\n",
    "\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "    f = math.sqrt((2 * math.pi) ** len(sigma)) * list_prod(sigma)\n",
    "    return f * np.exp(-scaled_sq_norm_diff(x, mu, sigma)/2)\n",
    "\n",
    "\n",
    "def sq_sum(x):\n",
    "    return sum((y**2 for y in x))\n",
    "\n",
    "\n",
    "def sq_norm_diff(x, y):\n",
    "    return sq_sum((z[0]-z[1] for z in zip(x, y)))\n",
    "\n",
    "\n",
    "def scaled_sq_norm_diff(x, y, s):\n",
    "    return sq_sum(((z[0]-z[1])/z[2] for z in zip(x, y, s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_responsibility(x, params):\n",
    "    probs = [w * gaussian(x, m, v) for w, m, v in params]\n",
    "    return [p/sum(probs) for p in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_terms(point, resp):\n",
    "    return [resp,\n",
    "            [[p * x for x in point] for p in resp],\n",
    "            [[p * x**2 for x in point] for p in resp]\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_products(prod1, prod2):\n",
    "    resp_sum = [r1 + r2 for r1, r2 in zip(prod1[0], prod2[0])]\n",
    "    weighted_point_sum = [[wx1 + wx2 for wx1, wx2 in zip(wp1, wp2)] for wp1, wp2 in zip(prod1[1], prod2[1])]\n",
    "    weighted_sq_point_sum = [[wsqx1 + wsqx2 for wsqx1, wsqx2 in zip(wsq1, wsq2)]\n",
    "                             for wsq1, wsq2 in zip(prod1[2], prod2[2])]\n",
    "    return [resp_sum, weighted_point_sum, weighted_sq_point_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errf(s1, s2):\n",
    "    return sum([sq_norm_diff(z1, z2) for z1, z2 in zip(s1, s2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em(k, ds, n, dim, comps, eps):\n",
    "    print(\"################################\")\n",
    "    print(\"EM started.\")\n",
    "    err = 1.\n",
    "    iter = 0\n",
    "    while err > eps:\n",
    "        sum_of_prods = data_parallel_region_with_reduction(ds,\n",
    "                                                           lambda x: weighted_terms(x, component_responsibility(x, comps)),\n",
    "                                                           aggregate_products,\n",
    "                                                           [(0.,) * k, [(0,) * dim] * k, [(0,) * dim] * k])\n",
    "        mix_weights = [w/n for w in sum_of_prods[0]]\n",
    "        means = [[y/z for y, z in zip(x[1], (x[0],) * dim)] for x in zip(sum_of_prods[0], sum_of_prods[1])]\n",
    "        vars = [[y[0]/y[1] - y[2]**2 for y in zip(x[0], (x[1],) * dim , x[2])]\n",
    "                for x in zip(sum_of_prods[2], sum_of_prods[0], means)]\n",
    "        last_comps = comps.copy()                                # save parameters\n",
    "        comps = [x for x in zip(mix_weights, means, vars)]       # update parameters\n",
    "        err = errf([x[1] for x in last_comps], [x[1] for x in comps])\n",
    "        iter += 1\n",
    "        print(\"iteration \", iter, \"completed.\")\n",
    "        print(\"################\")\n",
    "    return [err, comps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-10-e2b131645a9e>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e2b131645a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Data/mixture_3_unbalanced.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 341\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-10-e2b131645a9e>:1 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "data_file_name = \"Data/mixture_3_unbalanced.csv\"\n",
    "data = sc.textFile(data_file_name)\n",
    "sep = ','\n",
    "d = data.map(lambda l: list(map(lambda x: float(x), l.split(sep))))\n",
    "K = 3                    # Number of components\n",
    "dim = len(d.first())\n",
    "print(\"dimensionality = \", dim)\n",
    "n = d.count()\n",
    "print(\"data set size = \", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile = open(data_file_name)\n",
    "myreader = csv.reader(csvfile)\n",
    "d_list = [[float(x) for x in u] for u in myreader]\n",
    "if dim == 2:\n",
    "    sns.scatterplot(x=list(zip(*d_list))[0], y=list(zip(*d_list))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize distribution & mixture parameters\n",
    "# random.seed(1234567890)  # set a seed for testing\n",
    "# random weights\n",
    "mix_weights = [1./K] * K\n",
    "param_means = [[-2,2],[3,2],[-2,-2]]\n",
    "param_vars = [[.5,.5]] * K\n",
    "comps = [p for p in zip(mix_weights, param_means, param_vars)]\n",
    "print(\"Initial parameters: [[weight_1, mean_1, var_1], ..., [weight_k, mean_k, var_k]] =\")\n",
    "print(comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emresult = em(K, d, n, dim, comps, 0.01)\n",
    "print(emresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_components = emresult[1]\n",
    "weights, means, vars = list(zip(*(emresult[1])))\n",
    "means_x = list(zip(*means))[0]\n",
    "means_y = list(zip(*means))[1]\n",
    "if dim == 2:\n",
    "    sns.scatterplot(x=list(zip(*d_list))[0], y=list(zip(*d_list))[1])\n",
    "    sns.scatterplot(x=means_x, y=means_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
