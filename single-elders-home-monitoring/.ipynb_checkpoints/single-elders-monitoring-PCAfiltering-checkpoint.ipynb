{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14675974-036f-470f-9165-6eaadf923df8",
   "metadata": {},
   "source": [
    "# Single Elders Home Monitoring : PCA filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b35d6d-e527-47b2-8154-ead0c1342b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selecting framework for data management:\n",
    "PANDAS = False\n",
    "SPARK = True\n",
    "\n",
    "# needed for pyspark windows installs\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage import median_filter\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.offline as py\n",
    "import sklearn\n",
    "\n",
    "# import custom classes/libs\n",
    "from PCAWrapper import PCAWrapper\n",
    "\n",
    "# Enable offline mode\n",
    "import plotly.io as pio\n",
    "pio.renderers.keys()\n",
    "pio.renderers.default = 'jupyterlab' \n",
    "\n",
    "\n",
    "# spark\n",
    "if SPARK:\n",
    "    from pyspark.conf import SparkConf\n",
    "    from pyspark.sql import SparkSession \n",
    "\n",
    "    spark = (SparkSession.\n",
    "         builder.\n",
    "         master('local[*]'). # leave out for cluster mode\n",
    "         appName('single-elders-monitoring').\n",
    "         config(conf = SparkConf()).\n",
    "         getOrCreate())\n",
    "\n",
    "    from pyspark.sql.functions import col\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.types import FloatType\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    from pyspark.ml.feature import StandardScaler\n",
    "    from pyspark.sql.functions import col, concat, dayofmonth, hour, month, year\n",
    "    from pyspark.sql.functions import mean\n",
    "    from pyspark.sql.functions import lit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de105888-d118-4ff5-b97d-587e45346351",
   "metadata": {},
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f8a670-d934-4224-ae9b-3b7cd7627bec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe dataset with no occupants has \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m rows and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mno_occupants_df\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m SPARK:\n\u001b[1;32m----> 8\u001b[0m     no_occupants_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(no_occupants_url, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     no_occupants_df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe dataset with no occupants has \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m rows and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(no_occupants_df\u001b[38;5;241m.\u001b[39mcount(), \u001b[38;5;28mlen\u001b[39m(no_occupants_df\u001b[38;5;241m.\u001b[39mcolumns)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "no_occupants_url = './data/data_ref_until_2020-02-13.csv'\n",
    "\n",
    "if PANDAS:\n",
    "    no_occupants_df = pd.read_csv(no_occupants_url)\n",
    "    print(no_occupants_df)\n",
    "    print('The dataset with no occupants has {} rows and {} features.'.format(*no_occupants_df.shape))\n",
    "if SPARK:\n",
    "    no_occupants_df = spark.read.csv(no_occupants_url, header=True,inferSchema=True)\n",
    "    no_occupants_df.show()\n",
    "    print('The dataset with no occupants has {} rows and {} features.'.format(no_occupants_df.count(), len(no_occupants_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade6ba0-5aca-428f-8cdc-a051f54d9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupants_url = './data/database_gas.csv'\n",
    "\n",
    "if PANDAS:\n",
    "    occupants_df = pd.read_csv(occupants_url)\n",
    "    print(occupants_df)\n",
    "    print('The dataset with occupants has {} rows and {} features.'.format(*occupants_df.shape))\n",
    "          \n",
    "if SPARK:\n",
    "    occupants_df = spark.read.csv(occupants_url, header=True,inferSchema=True)\n",
    "    occupants_df.show()\n",
    "    print('The dataset with occupants has {} rows and {} features.'.format(occupants_df.count(), len(occupants_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ad940-83ae-4ae1-b56a-6aa6da3a6e09",
   "metadata": {},
   "source": [
    "## Sliding window median filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673f6ce-91c8-47bd-8338-c4ab181d3eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=11\n",
    "\n",
    "if PANDAS:\n",
    "    # filtering dataset of when no occupant is in the house\n",
    "    no_occupants_filtered_df = no_occupants_df.drop(columns=['timestamp']).rolling(window=window_size, \n",
    "                                                                                   min_periods=window_size//2 + 1,\n",
    "                                                                                   center=True).median()\n",
    "    no_occupants_filtered_df['timestamp'] = no_occupants_df['timestamp']\n",
    "    no_occupants_filtered_df = no_occupants_filtered_df.reset_index(drop=True)\n",
    "    print('The filtered dataset with no occupants has {} rows and {} features.'.format(*no_occupants_filtered_df.shape))\n",
    "\n",
    "    # filtering dataset with occupant in the house\n",
    "    occupants_filtered_df = occupants_df.drop(columns=['timestamp']).rolling(window=window_size, \n",
    "                                                                                   min_periods=window_size//2 + 1,\n",
    "                                                                                   center=True).median()\n",
    "    occupants_filtered_df['timestamp'] = occupants_df['timestamp']\n",
    "    occupants_filtered_df.dropna(inplace=True)\n",
    "    occupants_filtered_df.reset_index(inplace=True, drop=True)\n",
    "    print(occupants_filtered_df)\n",
    "    print('The filtered dataset with occupants has {} rows and {} features.'.format(*occupants_filtered_df.shape))\n",
    "\n",
    "if SPARK:\n",
    "    # defining window\n",
    "    windowSpec = Window.orderBy(\"timestamp\").partitionBy(\"day_month_year\").rowsBetween(-window_size//2, window_size//2) #Note: we can divide partition by hours to speed up computation\n",
    "\n",
    "    # Spark doesn't have a built-in implementation of median\n",
    "    rolling_median_udf = F.udf(lambda x: float(np.median(x)), FloatType())\n",
    "\n",
    "    cols_to_process = no_occupants_df.columns.copy()\n",
    "    cols_to_process.remove('timestamp')\n",
    "\n",
    "    conversion_dict = {column: rolling_median_udf(F.collect_list(column).over(windowSpec)) for column in cols_to_process}\n",
    "    \n",
    "    # filtering dataset of when no occupant is in the house\n",
    "    no_occupants_filtered_df = no_occupants_df.withColumn(\"day_month_year\", concat(dayofmonth(col(\"timestamp\")), month(col(\"timestamp\")), year(col(\"timestamp\"))))\n",
    "    no_occupants_filtered_df = no_occupants_filtered_df.withColumns(conversion_dict)\n",
    "    no_occupants_filtered_df.sort('timestamp').show()\n",
    "    print('The filtered dataset with no occupants has {} rows and {} features.'.format(no_occupants_filtered_df.count(), len(no_occupants_filtered_df.columns)))\n",
    "\n",
    "    # filtering dataset of when occupant is in the house\n",
    "    occupants_filtered_df = occupants_df.withColumn(\"day_month_year\", concat(dayofmonth(col(\"timestamp\")), month(col(\"timestamp\")), year(col(\"timestamp\"))))\n",
    "    occupants_filtered_df = occupants_filtered_df.withColumns(conversion_dict)\n",
    "    occupants_filtered_df.sort('timestamp').show()\n",
    "    print('The filtered dataset with occupants has {} rows and {} features.'.format(occupants_filtered_df.count(), len(occupants_filtered_df.columns)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb12ac-838b-40fc-86bf-3790f1221621",
   "metadata": {},
   "source": [
    "## PCA [fit] on no occupant data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b0bbd-5fec-4f7a-956c-68d6aa0ad21b",
   "metadata": {},
   "source": [
    "Unfortunately, the ```PCA``` implemented in Pyspark ML-lib has few features, we therefore define a ``PCAWrapper`` that, similarly to the ```scikit-learn``` PCA implementation, provides:\n",
    "1. **Support for centering** \n",
    "Normalizing the data just by using the mean, *not* rescaling standard deviation to 1:\n",
    "$$X_{centered} = X - \\overline{X}$$\n",
    "The fitting of the mean is done when calling the ```fit``` method.\n",
    "\n",
    "3. **Support to invert the transfromation PCA projection**\n",
    "We invert the transformation $$ X_{proj} = X_{centered}K$$\n",
    "as\n",
    "$$X_{centered_{reconstructed}} = X_{proj} \\cdot K^{-1} + \\overline{X} = X_{proj} \\cdot K^{T} + \\overline{X}$$\n",
    "\n",
    "Naturally, if $K$, is not a square matrix (output space has same dimension of input space), the reconstruction will be lossy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3190df0-426b-4f93-a003-6e19ea6c09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca fit\n",
    "n_components = 5\n",
    "\n",
    "if PANDAS:\n",
    "    \n",
    "    # IMPORTANT: scikit PCA implementation subtracts the mean of the input data as part of the function    \n",
    "    pca = PCA(n_components=n_components, whiten=True)\n",
    "    pca_no_occupants = pca.fit_transform(no_occupants_filtered_df.drop(columns=['timestamp']))\n",
    "\n",
    "    var_proj = pca.explained_variance_ratio_\n",
    "    \n",
    "    print('Variance explained by each dimension:\\n', \n",
    "          *[f'PCA component {i+1}: {round(var_proj[i]*100,2)} % \\n' for i in range (len(var_proj))])\n",
    "if SPARK:\n",
    "\n",
    "    # grouping features in an unique input column\n",
    "    assembler = VectorAssembler(inputCols = cols_to_process, outputCol = 'features')\n",
    "    assembled_df_no_occupants = assembler.transform(no_occupants_filtered_df).select('features')\n",
    "\n",
    "    # Define PCA model\n",
    "    pca = PCAWrapper(k=n_components, inputCol=\"features\", outputCol=\"pcaFeatures\", centering_data=True)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    pca = pca.fit(assembled_df_no_occupants)\n",
    "\n",
    "    # Transform the data\n",
    "    pca_no_occupants = pca.transform(assembled_df_no_occupants).select(\"pcaFeatures\")\n",
    "\n",
    "    var_proj = pca.explainedVariance\n",
    "    \n",
    "    print('Variance explained by each dimension:\\n', \n",
    "          *[f'PCA component {i+1}: {round(var_proj[i]*100,2)} % \\n' for i in range (len(var_proj))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604dfc81-8450-4629-aa10-c86473dd5ed5",
   "metadata": {},
   "source": [
    "## PCA inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e84896-d3ac-4718-9d69-7d9a9d078f8b",
   "metadata": {},
   "source": [
    "### No occupant data - before enviromental correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2972f85-85e4-474e-be8a-7d2500261539",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPARK:\n",
    "    # adapter from Pyspark to Pandas, just for visuals\n",
    "    no_occupants_filtered_df = no_occupants_filtered_df.select(\"*\").toPandas()\n",
    "    pca_no_occupants_ = pca_no_occupants.withColumn('pcaFeatures', vector_to_array('pcaFeatures'))\n",
    "    pca_no_occupants_ = np.array(pca_no_occupants.select(\"*\").toPandas()['pcaFeatures'].tolist())\n",
    "\n",
    "if PANDAS:\n",
    "    pca_no_occupants_ = pca_no_occupants\n",
    "    \n",
    "# Query the DataFrame for the date range\n",
    "start_date = np.datetime64('2020-01-27')\n",
    "end_date = np.datetime64('2020-02-01')\n",
    "no_occupants_filtered_df['timestamp'] = pd.to_datetime(no_occupants_filtered_df['timestamp']).dt.tz_localize(None)\n",
    "visual_days = ((no_occupants_filtered_df['timestamp'] >= start_date) & (no_occupants_filtered_df['timestamp'] <= end_date) & \n",
    "               ((no_occupants_filtered_df['timestamp'].dt.hour == 0) | (no_occupants_filtered_df['timestamp'].dt.hour == 4) | \n",
    "                (no_occupants_filtered_df['timestamp'].dt.hour == 8) | (no_occupants_filtered_df['timestamp'].dt.hour == 12) | \n",
    "                (no_occupants_filtered_df['timestamp'].dt.hour == 16) | (no_occupants_filtered_df['timestamp'].dt.hour == 20)))\n",
    "\n",
    "visual_data = pca_no_occupants_[visual_days]\n",
    "visual_data_days_df = pd.DataFrame({'PC1':visual_data[:,1], 'PC2':visual_data[:,0], 'day':pd.to_datetime(no_occupants_filtered_df['timestamp'][visual_days]).dt.date})\n",
    "visual_data_hours_df = pd.DataFrame({'PC1':visual_data[:,1], 'PC2':visual_data[:,0], 'hour':pd.to_datetime(no_occupants_filtered_df['timestamp'][visual_days]).dt.hour.astype('str')})\n",
    "\n",
    "\n",
    "## Plot no occupant data before environmental correction by hour\n",
    "x_min, x_max = visual_data_hours_df['PC1'].min()*1.05, visual_data_hours_df['PC1'].max()*1.05\n",
    "y_min, y_max = visual_data_hours_df['PC2'].min()*1.05, visual_data_hours_df['PC2'].max()*1.05\n",
    "\n",
    "fig = px.scatter(visual_data_hours_df, x='PC1', y='PC2', color='hour', opacity=0.5, color_discrete_sequence= px.colors.sequential.Plasma_r)\n",
    "fig.update_traces(marker=dict(colorscale='Viridis'), selector=dict(type='scatter'))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"PC1 ({:.2f} %)\".format(var_proj[0]*100),\n",
    "    yaxis_title=\"PC2 ({:.2f} %)\".format(var_proj[1]*100),\n",
    "    xaxis_range=[x_min, x_max],\n",
    "    yaxis_range=[y_min, y_max]  \n",
    ")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "## Plot no occupant data before environmental correction by day\n",
    "fig = px.scatter(visual_data_days_df, x='PC1', y='PC2', color='day', opacity=0.5, color_discrete_sequence= px.colors.sequential.Plasma_r)\n",
    "fig.update_traces(marker=dict(colorscale='Viridis'), selector=dict(type='scatter'))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"PC1 ({:.2f} %)\".format(var_proj[0]*100),\n",
    "    yaxis_title=\"PC2 ({:.2f} %)\".format(var_proj[1]*100),\n",
    "    xaxis_range=[x_min, x_max],\n",
    "    yaxis_range=[y_min, y_max]  \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c5408-4be9-4338-acf9-c19522c69b4c",
   "metadata": {},
   "source": [
    "PCA space representation of the data set without human activity before environmental correction. Samples colored by time of the day (top) and colored by day (bottom). Due to environmental factors, the samples are ordered by time of the day or by the day of acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993c476-f859-4043-b4aa-46a225165eb5",
   "metadata": {},
   "source": [
    "### No occupant data - after enviromental correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18233741-0f7d-4557-9908-fcee20fef023",
   "metadata": {},
   "source": [
    "We build another PCA just to visually investigate the effect on no occupant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fadd300-3cbe-413b-9eec-b177e57a437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new_components = 2\n",
    "\n",
    "if PANDAS:\n",
    "    # new PCA on same dataset without \n",
    "    #transformed_data_without_first_component = pca_no_occupants.copy()\n",
    "    # new PCA on filtered data\n",
    "    transformed_data_without_first_component = pca_no_occupants[visual_days].copy()\n",
    "    \n",
    "    # removing first component of PCA by setting it to zero\n",
    "    transformed_data_without_first_component[:,0] = 0\n",
    "\n",
    "    # reprojecting data in original space without first component\n",
    "    pca_after_corr = sklearn.decomposition.PCA(n_components=n_new_components)\n",
    "    inv_transf = pca.inverse_transform(transformed_data_without_first_component)\n",
    "\n",
    "    visual_data = pca_after_corr.fit_transform(inv_transf)\n",
    "    var_new_proj = pca_after_corr.explained_variance_ratio_\n",
    "\n",
    "if SPARK:\n",
    "    # unzipping vectors inside the single column to multiple columns\n",
    "    pca_no_occupants_unzipped = pca_no_occupants.select(['pcaFeatures']).withColumn(\"feature\", vector_to_array(\"pcaFeatures\")).select([col(\"feature\")[i] for i in range(n_components)])\n",
    "    \n",
    "    # zero-ing the first component of the pca, to remove it from re-projection in the original space\n",
    "    pca_no_occupants_unzipped = pca_no_occupants_unzipped.withColumn('feature[0]',lit(0))\n",
    "    \n",
    "    # re-zipping features into single column\n",
    "    assembler_invert = VectorAssembler(inputCols = pca_no_occupants_unzipped.columns, outputCol = 'pcaFeatures')\n",
    "    pca_no_occupants_zipped = assembler_invert.transform(pca_no_occupants_unzipped).select('pcaFeatures')\n",
    "    \n",
    "    # re-projecting to the original space\n",
    "    inv_transf = pd.DataFrame(pca.inverse_transform(pca_no_occupants_zipped))\n",
    "\n",
    "    # Define new PCA model\n",
    "    pca_after_corr = PCAWrapper(k=n_new_components, inputCol=\"pcaFeatures\", outputCol=\"pcaFeatures2\", centering_data=True)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    pca_after_corr = pca_after_corr.fit(pca_no_occupants_zipped)\n",
    "\n",
    "    # Transform the data\n",
    "    visual_data = pca_after_corr.transform(pca_no_occupants_zipped).select(\"pcaFeatures2\").toPandas()\n",
    "    visual_data = np.array(visual_data['pcaFeatures2'].tolist())\n",
    "\n",
    "    var_new_proj = pca_after_corr.explainedVariance\n",
    "    \n",
    "\n",
    "visual_data =visual_data[visual_days]\n",
    "\n",
    "visual_data_day_df = pd.DataFrame({'PC1':visual_data[:,1], 'PC2':visual_data[:,0], 'day':pd.to_datetime(no_occupants_filtered_df['timestamp'][visual_days]).dt.date})\n",
    "visual_data_hours_df = pd.DataFrame({'PC1':visual_data[:,1], 'PC2':visual_data[:,0], 'hour':pd.to_datetime(no_occupants_filtered_df['timestamp'][visual_days]).dt.hour.astype('str')})\n",
    "\n",
    "## Plot no occupant data after environmental correction by hour\n",
    "fig = px.scatter(visual_data_hours_df, x='PC1', y='PC2', color='hour', opacity=0.5, color_discrete_sequence= px.colors.sequential.Plasma_r)\n",
    "fig.update_traces(marker=dict(colorscale='Viridis'), selector=dict(type='scatter'))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"PC1 ({:.2f} %)\".format(var_new_proj[0]*100),\n",
    "    yaxis_title=\"PC2 ({:.2f} %)\".format(var_new_proj[1]*100),\n",
    "    xaxis_range=[x_min, x_max],\n",
    "    yaxis_range=[y_min, y_max] \n",
    ")\n",
    "fig.show()\n",
    "\n",
    "## Plot no occupant data after environmental correction by day\n",
    "fig = px.scatter(visual_data_day_df, x='PC1', y='PC2', color='day', opacity=0.5, color_discrete_sequence= px.colors.sequential.Plasma_r)\n",
    "fig.update_traces(marker=dict(colorscale='Viridis'), selector=dict(type='scatter'))\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"PC1 ({:.2f} %)\".format(var_new_proj[0]*100),\n",
    "    yaxis_title=\"PC2 ({:.2f} %)\".format(var_new_proj[1]*100),\n",
    "    xaxis_range=[x_min, x_max],\n",
    "    yaxis_range=[y_min, y_max]   \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f884eaf-8381-48eb-9a12-ecce1b83d75d",
   "metadata": {},
   "source": [
    "PCA space representation of the data set without human activity after environmental correction. Samples colored by time of the day (top) and colored by day (bottom). The samples overlap each other, with a reduced structure on the environmental factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d3d9d-a2d0-4a8c-a090-271b0a711e00",
   "metadata": {},
   "source": [
    "### Occupant data - after environmental correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac97b5-4e00-42b5-bb16-fd59b277cee7",
   "metadata": {},
   "source": [
    "We project the data with occupants in the vector space obtained by PCA on no occupant data, and we remove the first component supposing that it corresponds to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c576a-cfff-4ecb-a7fc-6103b46ab6b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SPARK:\n",
    "    # grouping features in an unique input column\n",
    "    assembled_df_occupants = assembler.transform(occupants_filtered_df).select('features')\n",
    "    \n",
    "    # projecting features of occupant data into space generated by no occupant data\n",
    "    pca_occupants = pca.transform(assembled_df_occupants).select(\"pcaFeatures\")\n",
    "    \n",
    "    # unzipping vectors inside the single column to multiple columns\n",
    "    pca_occupants_unzipped = pca_occupants.select(['pcaFeatures']).withColumn(\"feature\", vector_to_array(\"pcaFeatures\")).select([col(\"feature\")[i] for i in range(n_components)])\n",
    "    \n",
    "    # zero-ing the first component of the pca, to remove it from re-projection in the original space\n",
    "    pca_occupants_unzipped = pca_occupants_unzipped.withColumn('feature[0]',lit(0))\n",
    "\n",
    "    # zipping vectors to a single colum\n",
    "    pca_occupants_zipped = assembler_invert.transform(pca_occupants_unzipped).select('pcaFeatures')\n",
    "\n",
    "    # reproject it to original space\n",
    "    inv_transf_occupants = spark.createDataFrame(pca.inverse_transform(pca_occupants_zipped), schema=cols_to_process)\n",
    "\n",
    "    inv_transf_occupants.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
